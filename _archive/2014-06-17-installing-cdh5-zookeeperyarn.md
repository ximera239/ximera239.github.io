---
title: 'Installing CDH5. ZooKeeper&#038;YARN'
author: Evgeny
layout: post
permalink: /2014-06-17-installing-cdh5-zookeeperyarn/
categories:
  - Big Data
tags:
  - CDH5
  - HDFS
  - Installation manual
  - YARN
  - ZooKeeper
---
In this post, I'll document the process of installing Cloudera CDH5 on a cluster. I'll be following the [CDH5 installation guide][1].

<!--more-->

**Preparations**

1. Install Java 7 on all nodes

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install yandex-jdk7
</pre>

Alternatively, you can [follow these instructions][2].

2. Add CDH5 repositories

<pre class="toolbar:1 lang:sh decode:true" title="sudo vi /etc/apt/sources.list.d/cloudera.list">deb [arch=amd64] http://archive.cloudera.com/cdh5/ubuntu/precise/amd64/cdh precise-cdh5 contrib
deb-src http://archive.cloudera.com/cdh5/ubuntu/precise/amd64/cdh precise-cdh5 contrib</pre>

3. Add the repository key

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ curl -s http://archive.cloudera.com/cdh5/ubuntu/precise/amd64/cdh/archive.key | sudo apt-key add -</pre>

4. Update package lists

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get update</pre>

(One of my nodes showed a large number of security errors, but our admins said that it's okay, "this repository is only for us"...)

**Install and Configure ZooKeeper**

1. Install ZooKeeper

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install zookeeper
$ sudo apt-get install zookeeper-server</pre>

2. Create the ZooKeeper configuration file, for example at /var/lib/zookeeper/zookeeper.conf

<pre class="toolbar:1 lang:sh decode:true" title="sudo vi /var/lib/zookeeper/zookeeper.conf">tickTime=2000
dataDir=/var/lib/zookeeper/
clientPort=2181
initLimit=5
syncLimit=2
server.1=dev26i.vs.os.yandex.net:2888:3888
server.2=dev27i.vs.os.yandex.net:2888:3888
server.3=dev28i.vs.os.yandex.net:2888:3888

</pre>

3. Change the owner:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo chown -R zookeeper /var/lib/zookeeper/</pre>

4. Start each ZooKeeper instance (myid should be set according to step 6)

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo service zookeeper-server init --myid=1
$ sudo service zookeeper-server start
</pre>

5. Test ZooKeeper  
For example, from node2 to node1:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ zookeeper-client -server dev26i.vs.os.yandex.net:2181</pre>

**Install CDH5 Packages and Set Roles**

Following the [YARN architecture][3], I'll set up these roles:

* Node1: Resource Manager host, DataNode host, NodeManager host, MapReduce
* Node2: NameNode host, DataNode host, NodeManager host, MapReduce
* Node3: Secondary NameNode host, DataNode host, NodeManager host, MapReduce, History server, Proxy server

Node1:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install hadoop-yarn-resourcemanager</pre>

Node2:

<pre class="show-lang:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install hadoop-hdfs-namenode</pre>

Node3:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install hadoop-hdfs-secondarynamenode
...
Exception in thread "main" java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.
at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:350)
at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:338)
at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:331)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:230)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.&lt;init&gt;(SecondaryNameNode.java:192)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:651)
invoke-rc.d: initscript hadoop-hdfs-secondarynamenode, action "start" failed.</pre>

I'll address this later.

On DataNodes (all servers in our cluster - in a larger cluster, the Resource Manager could be separate):

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install hadoop-yarn-nodemanager hadoop-hdfs-datanode hadoop-mapreduce</pre>

On one server (for example, on the third node):

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install hadoop-mapreduce-historyserver hadoop-yarn-proxyserver

Setting up hadoop-yarn-proxyserver (2.3.0+cdh5.0.1+567-1.cdh5.0.1.p0.46~precise-cdh5.0.1) ...
* Starting Hadoop proxyserver:
starting proxyserver, logging to /var/log/hadoop-yarn/yarn-yarn-proxyserver-dev28i.vs.os.yandex.net.out
invoke-rc.d: initscript hadoop-yarn-proxyserver, action "start" failed.

</pre>

On client hosts (for our case, I'll install on the cluster's servers):

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo apt-get install hadoop-client</pre>

Now deploying hosts:

**Deploying HDFS on a Cluster** (from [this guide][4])  
1. Copy the default configuration to your custom directory

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo cp -r /etc/hadoop/conf.empty /etc/hadoop/conf.my_cluster</pre>

2. Set alternatives:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.my_cluster 50
$ sudo update-alternatives --set hadoop-conf /etc/hadoop/conf.my_cluster</pre>

3. Update configuration files

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ cd /etc/hadoop/conf.my_cluster
</pre>

<pre class="lang:sh decode:true">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://dev27i.vs.os.yandex.net:8020&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre>

<pre class="toolbar:1 lang:default decode:true" title="sudo vi hdfs-site.xml">&lt;configuration&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
     &lt;value&gt;file:///data/1/dfs/nn,file:///nfsmount/dfs/nn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
     &lt;value&gt;file:///data/1/dfs/dn,file:///data/2/dfs/dn,file:///data/3/dfs/dn,file:///data/4/dfs/dn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;
     &lt;value&gt;hadoop&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.failed.volumes.tolerated&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre>

Create directories and set owner and permissions:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo mkdir -p /data/1/dfs/nn
$ sudo mkdir -p /data/1/dfs/dn /data/2/dfs/dn /data/3/dfs/dn /data/4/dfs/dn
$ sudo chown -R hdfs:hdfs /data/1/dfs/nn /data/1/dfs/dn /data/2/dfs/dn /data/3/dfs/dn /data/4/dfs/dn
$ sudo chmod 700 /data/1/dfs/nn /data/1/dfs/dn /data/2/dfs/dn /data/3/dfs/dn /data/4/dfs/dn</pre>

See the [Setup NFS][5] post to learn how to set up NFS.

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo -u hdfs mkdir -p /nfsmount/dfs/nn
$ sudo -u hdfs chmod 700 /nfsmount/dfs/nn</pre>

4. Format the NameNode

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo -u hdfs hdfs namenode -format</pre>

5. Configure the Secondary NameNode

Add the Secondary NameNode host (or hosts, one per line):

<pre class="toolbar:1 lang:sh decode:true" title="sudo vi /etc/hadoop/conf.my_cluster/masters">dev28i.vs.os.yandex.net</pre>

<pre class="toolbar:1 lang:default decode:true" title="sudo vi hdfs-site.xml">&lt;property&gt;
    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
    &lt;value&gt;dev26i.vs.os.yandex.net:50070&lt;/value&gt;
    &lt;description&gt;
      The address and the base port on which the dfs NameNode Web UI will listen.
    &lt;/description&gt;
  &lt;/property&gt;</pre>

Set storage-balancing:

<pre class="toolbar:1 lang:default decode:true" title="sudo vi hdfs-site.xml">&lt;property&gt;
    &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold&lt;/name&gt;
    &lt;value&gt;10737418240&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction&lt;/name&gt;
    &lt;value&gt;0.75&lt;/value&gt;
  &lt;/property&gt;</pre>

**Start HDFS on the Cluster**

1. Configure folders on the second and third nodes (DataNode folders):

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo mkdir -p /data/1/dfs/dn /data/2/dfs/dn /data/3/dfs/dn /data/4/dfs/dn
$ sudo chown -R hdfs:hdfs /data/1/dfs/dn /data/2/dfs/dn /data/3/dfs/dn /data/4/dfs/dn
$ sudo chmod 700 /data/1/dfs/dn /data/2/dfs/dn /data/3/dfs/dn /data/4/dfs/dn</pre>

2. Replicate configurations to other nodes:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ scp -r /etc/hadoop/conf.my_cluster dev27i.vs.os.yandex.net:/etc/hadoop/conf.my_cluster
$ scp -r /etc/hadoop/conf.my_cluster dev28i.vs.os.yandex.net:/etc/hadoop/conf.my_cluster</pre>

3. Set up alternatives on the other nodes:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.my_cluster 50
$ sudo update-alternatives --set hadoop-conf /etc/hadoop/conf.my_cluster</pre>

4. Start HDFS:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done</pre>

5. Create a /tmp directory on HDFS:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo -u hdfs hadoop fs -mkdir /tmp
$ sudo -u hdfs hadoop fs -chmod -R 1777 /tmp</pre>

**Configure YARN**

1. Add to mapred-site.xml:

<pre class="toolbar:1 lang:default decode:true" title="sudo vi mapred-site.xml">&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
  &lt;/property&gt;
</pre>

2. Set up yarn-site.xml:

<pre class="toolbar:1 lang:default decode:true crayon-selected" title="sudo vi yarn-site.xml">&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;dev26i.vs.os.yandex.net&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;description&gt;List of directories to store localized files in.&lt;/description&gt;
    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;
    &lt;value&gt;file:///data/1/yarn/local,file:///data/2/yarn/local,file:///data/3/yarn/local&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;description&gt;Where to store container logs.&lt;/description&gt;
    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
    &lt;value&gt;file:///data/1/yarn/logs,file:///data/2/yarn/logs,file:///data/3/yarn/logs&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;description&gt;Where to aggregate logs to.&lt;/description&gt;
    &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
    &lt;!-- In Cloudera documentation you might see: &lt;value&gt;hdfs://var/log/hadoop-yarn/apps&lt;/value&gt; but this is wrong --&gt;
    &lt;value&gt;/var/log/hadoop-yarn/apps&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;description&gt;Classpath for typical applications.&lt;/description&gt;
     &lt;name&gt;yarn.application.classpath&lt;/name&gt;
     &lt;value&gt;
        $HADOOP_CONF_DIR,
        $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
        $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
        $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
        $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
     &lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre>

3. Set up folders and permissions:

<pre class="toolbar:2 nums:false lang:sh highlight:0 decode:true">$ sudo mkdir -p /data/1/yarn/local /data/2/yarn/local /data/3/yarn/local /data/4/yarn/local
$ sudo mkdir -p /data/1/yarn/logs /data/2/yarn/logs /data/3/yarn/logs /data/4/yarn/logs
$ sudo chown -R yarn:yarn /data/1/yarn/local /data/2/yarn/local /data/3/yarn/local /data/4/yarn/local
$ sudo chown -R yarn:yarn /data/1/yarn/logs /data/2/yarn/logs /data/3/yarn/logs /data/4/yarn/logs
</pre>

4. Configure the History Server in <samp class="ph codeph">mapred-site.xml</samp>:

<pre class="toolbar:1 lang:default decode:true" title="sudo vi mapred-site.xml">&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
    &lt;value&gt;dev28i.vs.os.yandex.net:10020&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
    &lt;value&gt;dev28i.vs.os.yandex.net:19888&lt;/value&gt;
  &lt;/property&gt;
</pre>

5. Set up proxying in <samp class="ph codeph">core-site.xml</samp>:

<pre class="toolbar:1 lang:default decode:true" title="sudo vi core-site.xml">&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;</pre>

6. Add staging directory in mapred-site.xml:

<pre class="toolbar:1 lang:default decode:true" title="sudo vi mapred-site.xml">&lt;property&gt;
    &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;
    &lt;value&gt;/user&lt;/value&gt;
&lt;/property&gt;</pre>

7. Sync configurations across the cluster.

8. Create the history directory:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo -u hdfs hadoop fs -mkdir -p /user/history
$ sudo -u hdfs hadoop fs -chmod -R 1777 /user/history
$ sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history
</pre>

9. Create the log directory:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn
$ sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn</pre>

10. Verify the file structure:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo -u hdfs hadoop fs -ls -R /

drwxrwxrwt   - hdfs hadoop          0 2014-06-17 08:52 /tmp
drwxr-xr-x   - hdfs hadoop          0 2014-06-17 11:56 /user
drwxrwxrwt   - mapred hadoop          0 2014-06-17 11:56 /user/history
drwxr-xr-x   - hdfs   hadoop          0 2014-06-17 11:58 /var
drwxr-xr-x   - hdfs   hadoop          0 2014-06-17 11:58 /var/log
drwxr-xr-x   - yarn   mapred          0 2014-06-17 11:58 /var/log/hadoop-yarn</pre>

11. Start/restart the Resource Manager:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo service hadoop-yarn-resourcemanager start
# or
$ sudo service hadoop-yarn-resourcemanager restart</pre>

12. Start/restart all NodeManagers:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo service hadoop-yarn-nodemanager start
# or
$ sudo service hadoop-yarn-nodemanager restart</pre>

13. Start/restart the History Server:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo service hadoop-mapreduce-historyserver start
# or
$ sudo service hadoop-mapreduce-historyserver restart</pre>

14. Add the current user to HDFS:

<pre class="toolbar:2 nums:false lang:default highlight:0 decode:true">$ sudo -u hdfs hadoop fs -mkdir /user/$USER
$ sudo -u hdfs hadoop fs -chown $USER /user/$USER</pre>

&nbsp;

Everything appears to be working correctly.

 [1]: http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/CDH5-Installation-Guide.html
 [2]: https://help.ubuntu.com/community/Java#Oracle_Java_7
 [3]: http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/YARN.html
 [4]: http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hdfs_cluster_deploy.html#topic_11_2_2_unique_1
 [5]: http://blog.forteamwork.ru/?p=39